{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [What is PyTorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Construct a 5x3 matrix, unitialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3032e+26,  4.5832e-41, -1.3032e+26],\n",
      "        [ 4.5832e-41,         nan,  0.0000e+00],\n",
      "        [ 7.6194e+31,  1.5564e+28,  4.7984e+30],\n",
      "        [ 6.2121e+22,  1.8370e+25,  1.4603e-19],\n",
      "        [ 6.4069e+02,  2.7489e+20,  1.5444e+25]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A randomly initialized matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3105, 0.5768, 0.4052],\n",
      "        [0.7626, 0.5865, 0.5171],\n",
      "        [0.3287, 0.7484, 0.2569],\n",
      "        [0.8918, 0.7016, 0.8462],\n",
      "        [0.3458, 0.8036, 0.2743]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor based on an existing tensor.\n",
    "New properties can be supplied when copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1035,  1.1275, -0.2697],\n",
      "        [-0.4714,  0.2587, -1.4718],\n",
      "        [ 0.6178,  1.1679,  0.9228],\n",
      "        [-0.6413, -0.8245,  0.8542],\n",
      "        [ 0.5625, -1.1115,  0.2299]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can get size of a tensor.\n",
    "It is returned as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "\n",
    "There are multiple syntaxes for operations.\n",
    "For example, addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0847,  1.3182,  0.4889],\n",
      "        [-0.2606,  0.7407, -1.2082],\n",
      "        [ 1.5850,  1.2210,  1.8451],\n",
      "        [-0.4966, -0.5162,  1.2255],\n",
      "        [ 0.7382, -1.0366,  0.8779]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0847,  1.3182,  0.4889],\n",
       "        [-0.2606,  0.7407, -1.2082],\n",
       "        [ 1.5850,  1.2210,  1.8451],\n",
       "        [-0.4966, -0.5162,  1.2255],\n",
       "        [ 0.7382, -1.0366,  0.8779]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0847,  1.3182,  0.4889],\n",
      "        [-0.2606,  0.7407, -1.2082],\n",
      "        [ 1.5850,  1.2210,  1.8451],\n",
      "        [-0.4966, -0.5162,  1.2255],\n",
      "        [ 0.7382, -1.0366,  0.8779]])\n"
     ]
    }
   ],
   "source": [
    "# provide an output tensor as an argument\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0847,  1.3182,  0.4889],\n",
      "        [-0.2606,  0.7407, -1.2082],\n",
      "        [ 1.5850,  1.2210,  1.8451],\n",
      "        [-0.4966, -0.5162,  1.2255],\n",
      "        [ 0.7382, -1.0366,  0.8779]])\n"
     ]
    }
   ],
   "source": [
    "# in place\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy-like indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1275,  0.2587,  1.1679, -0.8245, -1.1115])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing and reshaping using `torch.view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1 is inferred from the other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.item()` on a one-element tensor to get the Python number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3706])\n",
      "-0.3705705404281616\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and back is easy.\n",
    "If the Torch Tensor is on CPU, they point to the same part of memory and changes to one will change the other.\n",
    "\n",
    "#### Converting a Torch Tensor to a NumPy Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)  # add 1 in-place\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting NumPy Array to Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "\n",
    "Tensors can be moved onto any device using the `.to()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "\n",
    "    x = x.to(device)\n",
    "\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [Autograd: Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#)\n",
    "\n",
    "The `autograd` package provides the automatic differentiation that is central to all PyTorch neural networks.\n",
    "It is \"define-by-run\" framework meaning that the backpropogation is defined by how the code is run.\n",
    "This means that each iteration can be different.\n",
    "\n",
    "### Tensor\n",
    "\n",
    "The `torch.Tensor` is the central class of the package.\n",
    "If `.requires_grad` is set to `True`, all operations on it will be tracked.\n",
    "Once the computation is finished, `.backward()` can be called to have the gradient computed automatically.\n",
    "The gradient is accumulated into the `.grad` attribute.\n",
    "\n",
    "`.detach()` can be called on a tensor to stop tracking during a computation.\n",
    "\n",
    "To prevent tracking history (and using memory), the code can be wrapped `with torch.no_grad():`.\n",
    "This can be helpful when eveluating a model that has trainable parameters that require `requires_grad=True`, but the gradients are actually needed.\n",
    "\n",
    "The `Function` class is also very important for the `autograd` package.\n",
    "`Tensor` and `Function` build an acyclic graph with a complete hisotry of the computation.\n",
    "Each tensor has a `.grad_fn` attribute that references a `Function` that created the `Tensor`\n",
    "\n",
    "A call to `.backward()` will compute the derivatives of a `Tensor`.\n",
    "Unless the tensor is a scalar (holds a single element of data), another tensor must be supplied to the `gradient` argument.\n",
    "\n",
    "Create a tesnor and set `.requires_grad=True` to track computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a tensor operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` was created as a result of an operation, so it has a `grad_fn` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fc35ecbbdc0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform more operations on `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "Now we can perform backpropagation.\n",
    "\n",
    "Since `out` is a single scalar, we can call `.backward()` without passing any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient $\\frac{d(out)}{dx}$ can now be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "Neural networks (NN) are constructed using the `torch.nn` package.\n",
    "It uses `autograd` to define and differentiate them.\n",
    "An `nn.Module` contains layers and a method `.forward(input)` that returns the output.\n",
    "\n",
    "A typical training procedure:\n",
    "\n",
    "1. define the NN with some learnable parameters (weights)\n",
    "2. iterate over the dataset of inputs\n",
    "3. process the inputs through the network\n",
    "4. compute the loss\n",
    "5. propagate gradients back into the network's parameters\n",
    "6. update the weights of the network\n",
    "\n",
    "A typical simple updating rule for the final step is `weight = weight - learning_rate * gradient`.\n",
    "\n",
    "### Define the network\n",
    "\n",
    "Below is a example neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input channel, 6 output channels, 3x3 square convolution\n",
    "        \n",
    "        #kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6x6 from image dimensions\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2x2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward()` method must be defined and the `backward()` method is automatically defined using `autograd`.\n",
    "\n",
    "The learnable parameters of a model are returned by `net.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # the weights of conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try feeding in a random 32x32 input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0503,  0.0191,  0.0587,  0.0462, -0.0709, -0.1065,  0.0905, -0.0233,\n",
      "          0.1208, -0.0207]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `torch.nn` only supports mini-batches of input, not single samples.\n",
    "`input.unsqueeze(0)` can be used to fake batch dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "A loss function takes the model's output and target as inputs and computes a value that estimates how far away the outputs are from the target.\n",
    "\n",
    "There are several available in the `nn` package.\n",
    "A simple one is `nn.MSELoss` which computes the mean-squared error between the input and target.\n",
    "Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6354, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)  # mimic the shape of the output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss and all Tensors in the graph (that has `requires_grad=True`) will have their `.grad` Tensor accumulated with the gradient\n",
    "\n",
    "### Backprop\n",
    "\n",
    "To backprogagate the error, all that is needed is to call `loss.backward()`.\n",
    "Make sure to clear the existing gradients, first, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Below, we call `loss.backward()` and look at `conv1`'s bias gradients before and after the backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward.\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward.\n",
      "tensor([-0.0052, -0.0034,  0.0013, -0.0077,  0.0130,  0.0208])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print(\"conv1.bias.grad before backward.\")\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward.\")\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights\n",
    "\n",
    "The simplest rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "```\n",
    "weight = weight - learning_rate * gradient\n",
    "```\n",
    "\n",
    "Various updating rules have been updatig in `torch.optim`.\n",
    "An example of using it is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()  # zero the gradient buffers\n",
    "\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # performs the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0038,  0.0008,  0.0008, -0.0175,  0.0033,  0.0206])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
    "\n",
    "### What about data?\n",
    "\n",
    "Generally, the data can be loaded as NumPy arrays using standard libraries and then converted into `torch.*Tensor` objects.\n",
    "Here are some standard libraries for these operations:\n",
    "\n",
    "- images: Pillow, OpenCV\n",
    "- audio: scipy, librosa\n",
    "- text: standard Python, NLTK, SpaCy\n",
    "\n",
    "The `torchvision` package was created specifically for vision tasks.\n",
    "It has data loaders for common data sets (e.g. Imagenet, CIFAR10, MNIST) and data transformers for images, vizualization: `torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "For the following tutorial, we will use the CIFAR10 dataset.\n",
    "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "### Training and image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
